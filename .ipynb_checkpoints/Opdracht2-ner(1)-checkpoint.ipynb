{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second Group Assignment: A Named Entity Recognizer for Dutch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "TableOfContents": 1
   },
   "source": [
    "## Contents\n",
    "\n",
    "[Introduction](#Introduction)  \n",
    "[0. Preparation: Training data](#0.-Preparation:-Training-data)  \n",
    "\n",
    "[1. Step 1: A minimal NER tagger for Dutch](#1.-Step-1:-A-minimal-NER-tagger-for-Dutch)  \n",
    "[2. Step 2: Turn it into a script](#2.-Step-2:-Turn-it-into-a-script)  \n",
    "[3. Pickling and unpickling successfully](#3.-Pickling-and-unpickling-successfully)  \n",
    "[4. Self-testing](#4.-Self-testing)  \n",
    "[5. Step 3: Improve the feature selection](#5.-Step-3:-Improve-the-feature-selection)  \n",
    "[6. Step 4: Compare machine learning engines](#6.-Step-4:-Compare-machine-learning-engines)  \n",
    "[7. Step 5: Performance evaluation](#7.-Step-5:-Performance-evaluation)  \n",
    "[8. Step 6: Submission](#8.-Step-6:-Submission)  \n",
    "\n",
    "[9. Practicalities](#9.-Practicalities)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "* NLTK book, Chapter 6: Classification and classifiers\n",
    "* NLTK book, Chapter 7: Chunking and named entity recognition\n",
    "* Jurafsky and Martin, ch. 22.1, give a high-level introduction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this activity is to construct a Named Entity Recognizer\n",
    "(NER): A device that can scan natural text, identify named entities\n",
    "such as persons, places and organizations that are referred to by\n",
    "name, and classify them according to type (PERSON, LOCATION, etc.)\n",
    "\n",
    "You will train a classifier on the Dutch component of the CONLL2002\n",
    "corpus. The corpus also includes a Spanish\n",
    "component, so always specify which files you want to read.\n",
    "\n",
    "The necessary background concepts and software techniques are\n",
    "presented in chapter 7 of the NLTK book:\n",
    "\n",
    "[Section 7.2]: http://www.nltk.org/book/ch07.html#sec-chunking\n",
    "[Section 7.3]: http://www.nltk.org/book/ch07.html#developing-and-evaluating-chunkers\n",
    "[Section 7.5]: http://www.nltk.org/book/ch07.html#named-entity-recognition\n",
    "\n",
    "* [Section 7.2][] presents the concept of _chunking_, and how the NLTK\n",
    "manages its chunked corpora.\n",
    "\n",
    "* [Section 7.3][] shows how to build and evaluate chunkers with the\n",
    "help of the NLTK's chunked corpora. The discussion is based on the\n",
    "CONLL2000 corpus (note the year), a corpus of _English_ text in which\n",
    "all noun phrases are indicated.\n",
    "\n",
    "* Finally, [Section 7.5][] briefly covers the task of Named Entity\n",
    "Recognition. (Tip for the impatient: Sections 7.2 and 7.3 are\n",
    "essential reading--do not skip them).\n",
    "\n",
    "In the CONLL2002 corpus, which contains Spanish and Dutch components,\n",
    "only named entities have been chunked. Although the content of the\n",
    "chunks is different, the structure and interface of the corpora is the\n",
    "same: The text is annotated with POS tags, chunks, and chunk types.\n",
    "Thus the procedures for chunking noun phrases can be adapted to the NER task with\n",
    "minimal changes: Just train on the Dutch CONLL2002 corpus, and\n",
    "recognize _its_ chunks.\n",
    "\n",
    "* [Practicum 12 (Week 7)](https://uu.blackboard.com/webapps/blackboard/content/listContentEditable.jsp?content_id=_3158377_1&course_id=_120751_1) will explain a lot of what you need to know to do this assignment.\n",
    "-----\n",
    "\n",
    "**Note:** The nltk's classifiers and taggers need the external `numpy`\n",
    "library, but fail in a very confusing way if it is not found. The\n",
    "Anaconda distribution includes `numpy`, so that's not a problem unless\n",
    "you are using python without Anaconda."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Preparation: Training data\n",
    "\n",
    "We will once again use the `CONLL2002` corpus, which was specifically created for the task of named entity recognition.\n",
    "\n",
    "In earlier practica, we split the file `ned.train` into training\n",
    "and testing components. In fact, the corpus includes\n",
    "separate datasets for testing. Use all of the file `\"ned.train\"` (and nothing else) to train your models. Use the file `\"ned.testa\"` for testing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Step 1: A minimal NER tagger for Dutch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In brief:** Use the wrapper `custom_chunker.py`, provided below, to train a\n",
    "(trivial) named entity recognizer for Dutch. Pickle it and measure its performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Section 7.3.3](http://www.nltk.org/book/ch07.html#training-\n",
    "classifier-based-chunkers) of the NLTK book provides sample code for a\n",
    "chunker, showing how to wrap a sequential MaxEnt classifier in a\n",
    "converter that acccepts chunked sentences in `Tree` format.\n",
    "The code is intentionally simple, so we will use the following\n",
    "extended version. Save it as a module `custom_chunker.py`,\n",
    "and import it into your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package conll2002 to\n",
      "[nltk_data]     C:\\Users\\Jacco\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\conll2002.zip.\n"
     ]
    }
   ],
   "source": [
    "# FILE: custom_chunker.py\n",
    "\n",
    "# Natural Language Toolkit: code_classifier_chunker\n",
    "# Based on code from\n",
    "#   http://www.nltk.org/book/ch07.html#code-classifier-chunker\n",
    "#\n",
    "# Revisions:\n",
    "# - Not using \"megam\" as the machine learning engine.\n",
    "# - The feature builder is a constructor parameter.\n",
    "# - Added the method `explain()`, which prints the docstring of the feature builder.\n",
    "# - Added access to the `show_most_informative_features()` method of the underlying classifier.\n",
    "#\n",
    "# Alexis Dimitriadis\n",
    "\n",
    "import nltk\n",
    "from nltk.chunk.util import conlltags2tree, tree2conlltags\n",
    "\n",
    "nltk.download('conll2002')\n",
    "\n",
    "# If numpy is absent, the nltk fails with a very confusing error.\n",
    "# We avoid problems by checking directly\n",
    "try:\n",
    "    import numpy\n",
    "except ImportError:\n",
    "    print(\"You need to download and install numpy!!!\")\n",
    "    raise\n",
    "\n",
    "\n",
    "class ConsecutiveNPChunker(nltk.ChunkParserI):\n",
    "    \"\"\"\n",
    "    Train a classifier on chunked data in Tree format.\n",
    "    Arguments for the constructor:\n",
    "\n",
    "    featuremap   The function that will compute features for each word\n",
    "        in a sentence. See the NLTK book (and the assignment)\n",
    "        for the arguments it must accept.\n",
    "\n",
    "    train_sents  A list of sentences in chunked (Tree) format.\n",
    "    \n",
    "    algorithm  (optional). The name of the machine-learning model to use.\n",
    "    \"\"\"\n",
    "    def __init__(self, featuremap, train_sents, algorithm=\"IIS\"):\n",
    "        self._algorithm = algorithm\n",
    "        tagged_sents = [[((w,t),c) for (w,t,c) in tree2conlltags(sent)]\n",
    "                            for sent in train_sents]\n",
    "        self.tagger = _ConsecutiveNPChunkTagger(featuremap, tagged_sents, algorithm)\n",
    "\n",
    "    def parse(self, sentence):\n",
    "        tagged_sents = self.tagger.tag(sentence)\n",
    "        conlltags = [(w,t,c) for ((w,t),c) in tagged_sents]\n",
    "        return conlltags2tree(conlltags)\n",
    "\n",
    "    chunk = parse  # A synonym for the absent-minded\n",
    "    \n",
    "    def explain(self):\n",
    "        \"\"\"Print the docstring of our feature extraction function\"\"\"\n",
    "        print(\"Algorithm:\", self._algorithm)\n",
    "        # Print the feature map's help string:\n",
    "        print(self.tagger._featuremap.__doc__)\n",
    " \n",
    "    def show_most_informative_features(self, n=10):\n",
    "        \"\"\"Call our classifier's `show_most_informative_features()` function.\"\"\"\n",
    "        self.tagger.classifier.show_most_informative_features(n)\n",
    "\n",
    "\n",
    "class _ConsecutiveNPChunkTagger(nltk.TaggerI):\n",
    "    \"\"\"This class is not meant to be\n",
    "    used directly: Use ConsecutiveNPChunker instead.\"\"\"\n",
    "\n",
    "    def __init__(self, featuremap, train_sents, algorithm):\n",
    "        \n",
    "        self._featuremap = featuremap\n",
    "        train_set = []\n",
    "        for tagged_sent in train_sents:\n",
    "            untagged_sent = nltk.tag.untag(tagged_sent)\n",
    "            history = []\n",
    "            for i, (word, tag) in enumerate(tagged_sent):\n",
    "                featureset = self._featuremap(untagged_sent, i, history) \n",
    "                train_set.append( (featureset, tag) )\n",
    "                history.append(tag)\n",
    "        self.classifier = nltk.MaxentClassifier.train( \n",
    "            train_set, algorithm=algorithm, trace=0)\n",
    "\n",
    "    def tag(self, sentence):\n",
    "        history = []\n",
    "        for i, word in enumerate(sentence):\n",
    "            featureset = self._featuremap(sentence, i, history)\n",
    "            tag = self.classifier.classify(featureset)\n",
    "            history.append(tag)\n",
    "        return zip(sentence, history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use this module, you need a training dataset of chunked sentences\n",
    "(in Tree format) and a feature extractor function that will be used\n",
    "internally during training and regular use.\n",
    "\n",
    "A feature extractor must accept a tagged sentence `sentence`, the\n",
    "index `i` of a word in the sentence, and a list `history` containing\n",
    "the IOB tags that have already been assigned (presumably to earlier\n",
    "positions in the sentence). It must return a dictionary of the\n",
    "extracted features. Here is a very simple example, using just \n",
    "one feature out of all the available information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_features_1(sentence, i, history):\n",
    "    \"\"\"Simplest chunker features: Just the POS tag of the word\"\"\" \n",
    "    word, pos = sentence[i]\n",
    "    return { \"pos\": pos }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have our feature function, we can train a recognizer for the\n",
    "Dutch CONLL corpus as shown below.\n",
    "\n",
    "It takes a long time to train a recognizer (about half a second per\n",
    "sentence on my computer), so we demonstrate here with a tiny\n",
    "training set. Unsurprisingly,\n",
    "it's too small for the chunker to do anything useful with novel test\n",
    "data. Use larger datasets judiciously: Very short training sets are\n",
    "fine for checking if your code runs or crashes, but to find out if a new\n",
    "feature improves accuracy, you need to train on the entire dataset--\n",
    "or at least a substantial portion (several thousand sentences).\n",
    "\n",
    "If your code is too slow for anything but trivial datasets, figure out\n",
    "what is slowing it down. Pickle the trained model for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import conll2002 as conll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#from custom_chunker import ConsecutiveNPChunker\n",
    "\n",
    "tiny_sample = 150\n",
    "# training = conll.chunked_sents(\"ned.train\")  # Train with full dataset\n",
    "training = conll.chunked_sents(\"ned.train\")[:tiny_sample] # SHORT DATASET: FOR DEMO/DEBUGGING ONLY! \n",
    "testing = conll.chunked_sents(\"ned.testa\")\n",
    "\n",
    "simple_nl_NER = ConsecutiveNPChunker(simple_features_1, training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We evaluate our recognizer by calling its `evaluate()` method.\n",
    "Evaluation is a lot faster than training, so we use the entire test\n",
    "set, `ned.testa`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  De/Art\n",
      "  tekst/N\n",
      "  van/Prep\n",
      "  het/Art\n",
      "  arrest/N\n",
      "  is/V\n",
      "  nog/Adv\n",
      "  niet/Adv\n",
      "  schriftelijk/Adj\n",
      "  beschikbaar/Adj\n",
      "  maar/Conj\n",
      "  het/Art\n",
      "  bericht/N\n",
      "  werd/V\n",
      "  alvast/Adv\n",
      "  bekendgemaakt/V\n",
      "  door/Prep\n",
      "  een/Art\n",
      "  communicatiebureau/N\n",
      "  dat/Conj\n",
      "  (ORG Floralux/N)\n",
      "  inhuurde/V\n",
      "  ./Punc)\n",
      "(S\n",
      "  Dat/Pron\n",
      "  is/V\n",
      "  verder/Adj\n",
      "  opgelaaid/N\n",
      "  door/Prep\n",
      "  windsnelheden/N\n",
      "  die/Pron\n",
      "  oplopen/V\n",
      "  tot/Prep\n",
      "  35/Num\n",
      "  kilometer/N\n",
      "  per/Prep\n",
      "  uur/N\n",
      "  ./Punc)\n"
     ]
    }
   ],
   "source": [
    "print(conll.chunked_sents(\"ned.train\")[0])\n",
    "print(conll.chunked_sents(\"ned.testa\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  90.1%%\n",
      "    Precision:      7.7%%\n",
      "    Recall:         0.0%%\n",
      "    F-Measure:      0.1%%\n"
     ]
    }
   ],
   "source": [
    "print(simple_nl_NER.evaluate(testing))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsurprisingly, relying only on the part of speech is a very poor way\n",
    "to identify named entities. Our trivial recognizer finds a negligible\n",
    "proportion of all named entities (0% recall). Of the chunks it marks\n",
    "as named entities, a small proportion (7.7%) are indeed named\n",
    "entities; the rest were marked incorrectly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Step 2: Turn it into a script\n",
    "\n",
    "Training a non-trivial classifier is too time-consuming to keep\n",
    "entirely in a notebook. Prepare to work with python scripts (in IDLE\n",
    "or in your favorite editor), as follows:\n",
    "\n",
    "1. Save the script `custom_chunker.py` (see above), without any\n",
    "modifications. It is an importable module that you can use in your\n",
    "script.\n",
    "\n",
    "2. Create a script `features.py` for your feature extractors. \n",
    "Put the definition of `features_simple_1()` there as a starter. \n",
    "(You should later add, and use, additional functions.)\n",
    "\n",
    "3. You can now import both modules, or parts of them, for use in a\n",
    "Notebook or in other scripts. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from custom_chunker import ConsecutiveNPChunker\n",
    "from features import features_simple_1\n",
    "\n",
    "myRecognizer = ConsecutiveNPChunker(features_simple_1, training)\n",
    "# etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Pickling and unpickling successfully"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have already seen how to pickle and reload a trained tagger.\n",
    "Working with a classifier is slightly more complicated, since its\n",
    "operation relies on code that we write and revise.  This requires some\n",
    "care to work correctly.\n",
    "\n",
    "It is important to understand that **pickling in python only stores\n",
    "data.** Pickled objects do not store python code for function or class\n",
    "definitions. To reload a pickled object, python must be able to find\n",
    "the definition of its class and of any functions the object refers to.\n",
    "\n",
    "1. During pickling, a record is made of the modules where the needed\n",
    "types and functions were defined.\n",
    "\n",
    "2. During unpickling, the types and functions are imported from the\n",
    "recorded modules and used with the reloaded objects.\n",
    "\n",
    "This is a bit of behind-the-screens magic, so you must take some care\n",
    "to avoid problems:\n",
    "\n",
    "2.  After you store a pickled object, you should not modify the\n",
    "functions and classes it depends on (i.e., `ConsecutiveNPChunker` and\n",
    "the feature extraction function it uses). **If you modify your feature \n",
    "function after pickling a model, the model will become invalid.**\n",
    "Your model may or may not cause runtime errors, but the statistics \n",
    "will be incorrect and you'll have to train and pickle a new\n",
    "version. Use a different name for each version of your feature extraction \n",
    "function (`chunkfeatures_2`, `big_features`, or whatever), so \n",
    "that unpickled models can retrieve the right function later.\n",
    "\n",
    "* Code can only be found in *named* modules, but the main script does\n",
    "not count as a regular named module (its name is always just\n",
    "`__main__`). If you define your feature function (e.g.,\n",
    "`chunkfeatures_1`) in your main script, pickle a model, and unpickle\n",
    "it from a _different_ script, python will not be able to find your\n",
    "function. The solution is simple:\n",
    "\n",
    "    * **All necessary classes and functions should be defined in\n",
    "modules (one or several), and *imported* into your main script.**\n",
    "\n",
    " The script that unpickles your model will then know where to find\n",
    "everything.\n",
    "\n",
    "\n",
    "\n",
    "**In short:** Use a different name for each new feature extraction\n",
    "function you define, and keep their definitions in modules, not in\n",
    "your main script.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Self-testing\n",
    "\n",
    "Here is a simple script that should be able to use your pickled\n",
    "tagger. Ensure that your code is compatible with it. If your code does\n",
    "not work with this script, **do not modify the script.** Fix your code\n",
    "so that it is compatible with the script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FILE: model_test.py\n",
    "\n",
    "import pickle\n",
    "ner = pickle.load(open(\"best.pickle\", \"rb\"))\n",
    "\n",
    "from nltk.corpus import conll2002 as conll\n",
    "\n",
    "# Usage 1: parse a list of sentences (with POS tags)\n",
    "tagzinnen = conll.tagged_sents(\"ned.train\")[1000:1050]\n",
    "result = ner.parse_sents(tagzinnen)\n",
    "\n",
    "# Usage 2: self-evaluate (on chunked sentences)\n",
    "chunkzinnen = conll.chunked_sents(\"ned.testa\")[1000:1500]\n",
    "print(ner.evaluate(chunkzinnen))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Step 3: Improve the feature selection\n",
    "\n",
    "**In brief:** Choose suitable features to improve your classifier's\n",
    "performance.\n",
    "\n",
    "Now that you have a minimal working classifier, define better \n",
    "feature functions to improve your classifier's performance. Train with\n",
    "`ned.train`, and evaluate performance on the data in `ned.testa` only.\n",
    "\n",
    "You can start with including capitalization information about the\n",
    "current word; you can also experiment with features about the number\n",
    "of letters in the word, the tag that precedes or follows it, whether\n",
    "it follows a word that is already marked as part of a named entity\n",
    "(use the `history` vector), etc.\n",
    "\n",
    "You are also allowed to use (some) external resources. E.g., our list\n",
    "of Dutch proper names (from an earlier activity) might be helpful in\n",
    "recognizing whether some word is a person's name: Add a feature that\n",
    "tells you whether a word is the list of names (or perhaps in an abbreviated\n",
    "list containing just the most common names). You may also use any dataset\n",
    "that is provided by the nltk (available through `nltk.download`). Ask\n",
    "us first before using any other, external resources.\n",
    "\n",
    "These are only suggestions: Utilize your reading and your imagination\n",
    "to come up with more.\n",
    "\n",
    "It is not necessary to **report** on every version of the feature\n",
    "extractors you try out, but you may include a demonstration of more\n",
    "than one if, e.g., you find that the \"best\" function depends on the algorithm (see\n",
    "next step)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Step 4: Compare machine learning engines\n",
    "\n",
    "There are many different learning algorithms for classifiers, and the\n",
    "NLTK offers several of them. The code in `custom_chunker.py` uses the\n",
    "MaxEnt classifier, which supports four kinds of maximum entropy\n",
    "optimization. You can select among them by using the `algorithm`\n",
    "argument of the `ConsecutiveNPChunker` constructor (which will be passed\n",
    "to `MaxEnt`). To see the available algorithms, print the attribute `ALGORITHMS`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "print(nltk.classify.MaxentClassifier.ALGORITHMS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The names stand for Generalized Iterative Scaling (\"GIS\"), Improved Iterative Scaling\n",
    "(\"IIS\", the default), and two that require external libraries:\n",
    "the Megam library (which uses the LM-BFGS algorithm), and the Toolkit\n",
    "for Advanced Discriminative Modeling (TADM).  The default is `'IIS'`.\n",
    "\n",
    "The NLTK also offers the NaiveBayes classifier, which can be used\n",
    "instead of MaxEnt. You'll need to **modify \n",
    "`custom_chunker.py`** so that it accepts `\"NaiveBayes\"` as a special \n",
    "value for the `algorithm` argument, and uses it instead of the MaxEnt classifier. \n",
    "\n",
    "Experiment with the different engines and algorithms, and find out\n",
    "which combination of algorithms and features gives the best\n",
    "performance on the test data. Note that the Megam and TADM algorithms\n",
    "require software that must be downloaded and installed separately. Put\n",
    "those aside and work with the rest: **`IIS, GIS,` and the NaiveBayes\n",
    "classifier. Document the performance of all three in your report.** (You should at\n",
    "least report on their performance with your \"best\" feature selection; \n",
    "but different features may perform best with different\n",
    "algorithms, so consider exploring that as well.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Step 5: Performance evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train your best, finished classifier on the complete set of sentences\n",
    "in `ned.train`, pickle it, and evaluate it on the data in `ned.testa`.\n",
    "Include the results in your short report (see Submission).\n",
    "\n",
    "Precision and recall are discussed in [section 6.3.3][3.3] of the NLTK\n",
    "book. The section immediately below that presents the NLTK's\n",
    "[\"ConfusionMatrix( )\"][3.4] method, which is useful for identifying\n",
    "where your classifier makes mistakes. (Its use is optional).\n",
    "\n",
    "You may also find the nltk's [nltk.chunk.util.ChunkScore(\n",
    ")](http://www.nltk.org/api/nltk.chunk.html#nltk.chunk.util.ChunkScore)\n",
    "function useful (again, optional).\n",
    "\n",
    "[3.3]: http://www.nltk.org/book/ch06.html#precision-and-recall\n",
    "[3.4]:  http://www.nltk.org/book/ch06.html#confusion-matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What performance should you aim for?** As a baseline, a very simple\n",
    "six-feature classifier achieved about 60% precision and recall,\n",
    "trained on the entire `ned.train` corpus and tested on `ned.testa`.\n",
    "Your solution should do at least this well.\n",
    "\n",
    "<code><pre>\n",
    "MODEL: Minimal classifier\n",
    "MaxEnt, IIS algorithm\n",
    "6 features, 15806 train sentences, 2895 testing\n",
    "\n",
    "ChunkParse score:\n",
    "    IOB Accuracy:  95.8%\n",
    "    Precision:     59.0%\n",
    "    Recall:        60.0%\n",
    "    F-Measure:     59.5%\n",
    "\n",
    "Should have found 2616 chunks, guessed 2661, correct 1564\n",
    "</pre></code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more complex classifier achieved almost 15% higher precision and\n",
    "a modest gain in recall. You can easily do even better with a bit of\n",
    "exploration. Note that you'll probably need to let the training run\n",
    "for hours, and possibly overnight or longer: Allow enough time to\n",
    "train and improve your models. Training on very short datasets is \n",
    "not useful for comparing between feature sets: there must be enough \n",
    "variety in the training data to weigh the features properly.\n",
    "\n",
    "<pre>\n",
    "MODEL: NLTK feature set\n",
    "MaxEnt, default algorithm\n",
    "15 features, 15806 train sentences, 2895 testing\n",
    "Training...  done, 24443.5 s (1.55s per sentence)\n",
    "\n",
    "ChunkParse score:\n",
    "    IOB Accuracy:  96.2%\n",
    "    Precision:     73.8%\n",
    "    Recall:        62.0%\n",
    "    F-Measure:     67.4%\n",
    "\n",
    "Should have found 2616 chunks, guessed 2199, correct 1616\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your classifier will also be evaluated on novel newstext in Dutch (to\n",
    "be distributed later)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Step 6: Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare to submit your code, organized in the following units (which\n",
    "may, and should, share some additional modules that you **also**\n",
    "submit).\n",
    "\n",
    "Double-check that the code and models you upload can run without\n",
    "modifications, and according to the instructions. Your code should\n",
    "contain no absolute paths.\n",
    "\n",
    "1. **Your python source files.** It should consist of the following\n",
    "standard files, plus any additional files you find it useful to\n",
    "create:\n",
    "\n",
    "    1. A script `BuildModels.py` that trains and pickles the different\n",
    "classifiers you have\n",
    "      developed. \n",
    "    2. A script `EvaluateModels.py` that loads and evaluates your\n",
    "models. Make sure it prints at least: something that identifies the model/feature set, and the precision, recall, and F-measure for each model. You may also find you want to print more things; use your discretion. These two scripts together are an abbreviated record of the\n",
    "approaches you have explored.\n",
    "    4. The module `features.py` that defines, or imports from other\n",
    "modules,\n",
    "       one or more feature extractor functions (whatever you find\n",
    "worth reporting on).\n",
    "    5. Your modified module `custom_chunker.py`.\n",
    "    3. The file `Evaluation-output.txt`, with the saved output of the\n",
    "evaluation.\n",
    "    5. Any additional files you require, including external data\n",
    "(e.g., a list of names).\n",
    "\n",
    " Ensure that the output of your evaluation script is a comprehensible report,\n",
    "not just raw numbers. E.g., print out a line identifying the\n",
    "classifier that you are about to `evaluate()`, and add some blank\n",
    "lines or other visual structure. Add a line of output identifying your\n",
    "best model.\n",
    "<p/>\n",
    "\n",
    "2. **The pickled model of your best classifier.** Please name it\n",
    "`best.pickle` and **test** that it can be reloaded and works correctly\n",
    "with the simple script `model_test.py` (see above). If you needed to make\n",
    "**any** changes to `model_test.py` to get it to run, be sure to include\n",
    "your version in the upload. **If you need to do this, it will cost you a few points**<p/>\n",
    "\n",
    "4. **A short report** (1-2 pages, **txt file or PDF only**), summarizing your\n",
    "results. (Cf. the results from step 4.)  Report the features and\n",
    "classifier engine you used; precision, recall and F-value; how much\n",
    "time was needed for training and for evaluation of your best model;\n",
    "and any additional explanations or observations about the task.\n",
    "\n",
    "<p/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Practicalities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* There will be a small grade bonus (and bragging rights!) for the three\n",
    "groups that achieve the highest F-scores on new data.\n",
    "\n",
    "\n",
    "*    Your work should be your own. Attribute external sources etc. Do not ask for help in any forum.\n",
    "\n",
    "\n",
    "*    You can freely use any part of the conll module itself and any\n",
    "other\n",
    "    parts of the NLTK, including utility routines for evaluation, etc.\n",
    "    You may use standard python libraries, and all libraries included with\n",
    "Anaconda; but not libraries that must be\n",
    "    separately downloaded, except with prior approval. If in doubt,\n",
    "ask us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
